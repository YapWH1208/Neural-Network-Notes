# 理论
- 由感知机所构成的多重感知机神经网络在处理线性问题的时候不会变现出问题，但是当问题是一个非线性问题时，多重感知机无法很好地拟合。
- 而激活函数便是一个良药（线性变换函数），它能让神经网络拥有拟合非线性问题的能力。

# 激活函数的条件
- 可连续求导的函数
- 能够覆盖所有实数 --> $\mathbb{R}$
- 单调递增的 $S$ 型曲线（随着 $y$ 而变化）

# 激活函数的种类
## Sigmoid
$$\large \sigma = {{1}\over{1+e^{-y}}}$$
- 定义域 $[0,1]$
	- 非零均值函数，回导致收敛速度慢
- 导数最大值：0.25
	- 导致反向传播一直递减
	- 饱和函数，取值有上限
		- 遇到极限导数链时会导致梯度消失

## Tanh
$$\large \tanh = {{1 - e^{-y}}\over{1 + e^{-y}}}$$
- 定义域 $[-1,1]$
- 导数最大值：1
	- 饱和函数，取值有上限
		- 遇到极限导数链时会导致梯度消失

## ReLU
$$\text{ReLU}(x) = \begin{cases}
    0 & \text{if } x < 0 \\
    x & \text{if } x \geq 0
\end{cases}$$

- 算法简单
	- 神经网络收敛速度快
	- 对于非零 $x$ 值，减少了计算量
- 非零均值函数
	- 可用归一化解决
- 取值无上限
	- 梯度爆炸现象 --> 数值初始化
- 非零 $x$ 值，会出现神经坏死现象
	- 使用优化版激活函数
		- Leaky ReLU ( $0.01x$ )
		- Parametric ReLU ( $\alpha x$ )
		- 等等

# 参考文献
- [[5分钟深度学习] #03 激活函数](https://www.bilibili.com/video/BV1qB4y1e7GJ/?vd_source=82cc9f8195ff57b14f4f1d470824ef31)
