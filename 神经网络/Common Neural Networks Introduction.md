# Types of Neural Networks
There are mainly two types of neural networks which are Feedforward neural network and Feedback neural network, it is identified by their basic structure.

## Feedforward Neural Network
In feedforward nueral network, data will go through the whole network from the input layer to the output layer layer by layer without looping into the same layer. So, the output of the neuronons will not affect the neurons in the same layer.

Common feedforward neural network includes convolutional neural network (CNN), fully connected neural network (FCN), generative adversarial network (GAN).

<div align="center">
  <img src="https://static.packt-cdn.com/products/9781788397872/graphics/1ebc2a0a-2123-4351-b7e1-eb57f098bafa.png" alt="Feedforward Neural Network">
  <p>Diagram 1: Feedforward Neural Network</p>
</div>
## Feedback Neural Network
In Feedback Neural Network, neurons can accept input from the previous layer and also the feedback signal from itself. Comparing with feedforward neural network, neurons in feedback neural network have memory, it can learn different states in different timestep.

Propagation process in feedback neural networks can be one-way or two-way, so it can be represented by a directed cycle graph or an undirected graph.

Common feedback neural network includes recurrent neural network (RNN), long short term memory (LSTM), Hopfield network.

<div align="center">
  <img src="https://pic4.zhimg.com/v2-4d0ecf7ebdfae66e1274c10de1ef482f_r.jpg" alt="Feedback Neural Network">
  <p>Diagram 2: Feedback Neural Network</p>
</div>

# Common Neural Network Models Introduction
## Fully Connected Neural Network
Fully connected neural network is the most common network structure for deep learning. It has three types of layers: input layer, hidden layer and output layer. Each neuron in the current layer is connected to the input signal of each neuron in the previous layer. 

In each connection process, the signal from the previous layer is multiplied by a weight, a bias is added, and then passed through a nonlinear activation function.

<div align="center">
  <img src="https://orgs.mines.edu/daa/wp-content/uploads/sites/38/2019/08/1_Gh5PS4R_A5drl5ebd_gNrg@2x.jpg" alt="Fully Connected Neural Network">
  <p>Diagram 3: Fully Connected Neural Network</p>
</div>

## Convolution Neural Network
Images are of very high dimensionality, so training a standard feedforward network to recognize images would require tens of thousands of input neurons, which in addition to the obvious high computational effort, could lead to many problems related to the curse of dimensionality in neural networks. 

Convolutional neural networks provide a solution that uses convolution and pooling layers to reduce the dimensionality of images. Since the convolutional layer is trainable but has significantly fewer parameters than a standard hidden layer, it is able to highlight important parts of the image and propagate each important part forward. 

In traditional CNNs, the last few layers are hidden layers, which are used to process "compressed image information".

<div align="center">
  <img src="https://editor.analyticsvidhya.com/uploads/67201cnn.jpeg" alt="Convolution Neural Network">
  <p>Diagram 4: Convolution Neural Network</p>
</div>

## Residual Network
There is a problem with deep feedforward neural networks. As the number of network layers increases, the network will undergo degradation: as the number of network layers increases, the training set loss gradually decreases, and then tends to be saturated. When the network depth is increased, The training set loss will increase instead. 

To solve this problem, residual networks use skip connections to achieve cross-layer signal propagation.

<div align="center">
  <img src="https://www.researchgate.net/publication/350524328/figure/fig1/AS:1007436949364737@1617203094867/Resnet-Architectures-Right-And-Residual-Block-Top-Left-Bottleneck-Layer-Bottom.ppm" alt="Residual Network">
  <p>Diagram 5: Residual Network</p>
</div>

## Generative Adversarial Network
Generative adversarial network is a network specifically designed to generate images and consists of two networks: a discriminator and a generator. 

The discriminator's task is to distinguish whether an image was extracted from the dataset or generated by the generator, and the generator's task is to generate images that are realistic enough that the discriminator cannot distinguish whether the image is real or not. 

Over time, and under careful supervision, the two rivals compete with each other, each trying to successfully improve the other. The end result is a well-trained generator that produces realistic images. 

The discriminator is a convolutional neural network whose goal is to maximize the accuracy of identifying real and fake images, while the generator is a deconvolutional neural network whose goal is to minimize the performance of the discriminator.

<div align="center">
  <img src="https://www.researchgate.net/profile/Bhaskar-Ghosh/publication/344544069/figure/download/fig1/AS:944366453542923@1602165916976/Generative-Adversarial-Network-Architecture-2.jpg" alt="Generative Adversarial Network">
  <p>Diagram 6: Generative Adversarial Network</p>
</div>

## Variational Autoencoder
Autoencoders learn a compressed representation of an input (which can be an image or text sequence), e.g., compress the input and then decompress it back to match the original input, while variational autoencoders learn the parameters of a probability distribution that represents the data. Rather than just learning a function that represents the data, it obtains a more detailed and granular view of the data, sampling from the distribution and generating new input data samples.

<div align="center">
  <img src="https://www.compthree.com/images/blog/ae/vae.png" alt="Variational Autoencoder">
  <p>Diagram 7: Variational Autoencoder</p>
</div>

## Transformer
Transformer is a classic network structure proposed by Google Brain, which consists of the classic Encoder-Decoder model. In the picture below, the entire Encoder layer consists of 6 structures of the N parts on the left. The entire Decoder is composed of 6 frames of the N parts on the right. After the Decoder output result is transformed by a linear layer, it is calculated by the softmax layer and the final prediction result is output.

<div align="center">
  <img src="https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png" alt="Transformer">
  <p>Diagram 8: Transformer</p>
</div>

## Recurrent Neural Network
Recurrent neural networks are a special type of network that contains loops and self-repetitions, hence the name "loop". By allowing information to be stored in the network, RNNs use reasoning from previous training to make better, more informed decisions about upcoming events. 

To do this, it uses previous predictions as "contextual signals". Due to their nature, RNNs are often used to handle sequential tasks, such as generating text verbatim or predicting time series data (such as stock prices). They can also handle input of any size.

<div align="center">
  <img src="https://kvitajakub.github.io/img/rnn-unrolled.png" alt="Recurrent Neural Network">
  <p>Diagram 9: Recurrent Neural Network</p>
</div>

## Long Short Term Memory
The Long Short Term Memory (LSTM) structure is specially designed to solve the problem of gradient disappearance and explosion when RNN learns long context information. Memory blocks are added to the structure. These modules can be thought of as memory chips in a computer - each module contains several cyclically connected memory cells and three gates (input, output and forget, equivalent to write, read and reset). Inputs of information can only interact with neurons through each gate, so the gates learn to intelligently open and close to prevent gradients from exploding or disappearing.

<div align="center">
  <img src="https://i.stack.imgur.com/voZql.jpg" alt="LSTM">
  <p>Diagram 10: Long Short Term Memory</p>
</div>

## Hopfield Network
Hopfield neural network is a single-layer feedback neural network that is fully connected to each other. Each neuron is both an input and an output. Each neuron in the network transmits its output to all other neurons through connection weights, and at the same time receives information from all other neurons.

<div align="center">
  <img src="https://image1.slideserve.com/2036297/hopfield-network-l.jpg" alt="Hopfield Network">
  <p>Diagram 11: Hopfield Network</p>
</div>

# Reference
- [有哪些常用的神经网络模型？](https://www.zhihu.com/question/447419811/answer/2189716236)