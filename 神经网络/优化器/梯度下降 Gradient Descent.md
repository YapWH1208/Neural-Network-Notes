>ç›²äººä¸‹å±±æ³•ğŸ‘¨â€ğŸ¦¯ğŸ‘¨â€ğŸ¦¯ğŸ‘¨â€ğŸ¦¯â€”â€”æ‹æ–å‘¨å›´æŒ¨ç€æ•²ä¸€åœˆï¼Œæ‰¾ä¸ªæœ€é™¡çš„å¡ä¸‹å±±  
>éšæœºç›²äººä¸‹å±±æ³•ğŸ‘©â€ğŸ¦¯â€”â€”çˆ·æ¯æ¬¡å°±æœç€æ‹æ–ç¬¬ä¸€ä¸‹éšä¾¿æ•²çš„å¡ä¸‹å±±  
>å°æ‰¹é‡ç›²äººä¸‹å±±æ³•ğŸ‘¨â€ğŸ¦¯ğŸ‘©â€ğŸ¦¯â€”â€”çˆ·æ‹æ–å‘¨å›´éšä¾¿æ•²å‡ ä¸‹ï¼Œä»é‡Œé¢æ‰¾ä¸ªæœ€é™¡çš„å¡ä¸‹

# æ¢¯åº¦ä¸‹é™
## æ¢¯åº¦ä¸‹é™
- åŒæ—¶ä½¿ç”¨å…¨éƒ¨å®ä¾‹ç‚¹æ›´æ–°å‚æ•°
- $n$ä¸ªè¿­ä»£

## éšæœºæ¢¯åº¦ä¸‹é™
- ä½¿ç”¨éšæœºä¸€ä¸ªå®ä¾‹ç‚¹æ›´æ–°å‚æ•°
- åœ¨ä¸€ä¸ªè¿­ä»£ä½¿ç”¨æ‰€æœ‰çš„å®ä¾‹ç‚¹
- $m\times n$è¿­ä»£

## å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
- ä½¿ç”¨éšæœºå‡ ä¸ªå®ä¾‹ç‚¹æ›´æ–°å‚æ•°
- ${m\over b}\times n$è¿­ä»£


# ä¼˜ç¼ºç‚¹

| GD     |  å› ç´         | SGD        |
| ------ | -------- | ---------- |
| æ…¢     | é€Ÿåº¦     | å¿«         |
| æ›´å‡†ç¡® | æ‹Ÿåˆ     | ä¸é‚£ä¹ˆå‡†ç¡® |
| å°     | å™ªå£°å½±å“ | å¤§         |
| å¤§     | å†…å­˜éœ€æ±‚ | å°         |


# ä»£ç å®ç°
```python
import numpy as np

# Gradient Descent (GD) for matrix data
def gradient_descent(X, y, learning_rate, num_iterations):
    m, n = X.shape
    theta = np.zeros(n)  # Initialize parameters
    
    for _ in range(num_iterations):
        # Compute predictions
        predictions = np.dot(X, theta)
        
        # Calculate gradient
        gradient = np.dot(X.T, predictions - y) / m
        
        # Update parameters
        theta -= learning_rate * gradient
    
    return theta

# Stochastic Gradient Descent (SGD) for matrix data
def stochastic_gradient_descent(X, y, learning_rate, num_iterations, batch_size):
    m, n = X.shape
    theta = np.zeros(n)  # Initialize parameters
    
    for _ in range(num_iterations):
        # Randomly shuffle the data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(0, m, batch_size):
            # Select mini-batch
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # Compute predictions
            predictions = np.dot(X_batch, theta)
            
            # Calculate gradient
            gradient = np.dot(X_batch.T, predictions - y_batch) / batch_size
            
            # Update parameters
            theta -= learning_rate * gradient
    
    return theta
```