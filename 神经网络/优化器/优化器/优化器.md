# 梯度下降 Gradient Descent
- 一次过使用所有的实例点进行梯度更新

```python
import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(X, y, learning_rate=0.01, epochs=100):
    m, n = X.shape
    theta = np.zeros((n, 1))
    loss_history = []

    for _ in range(epochs):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = X.T.dot(errors) / m
        theta -= learning_rate * gradient
        loss = np.mean(errors**2) / 2
        loss_history.append(loss)

    return theta, loss_history

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add a bias term to the feature matrix
X_b = np.c_[np.ones((100, 1)), X]

# Run gradient descent
theta_gd, loss_history_gd = gradient_descent(X_b, y)

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history_gd, label='Gradient Descent')
plt.title('Loss over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

<div align="center">
  <img src="https://github.com/YapWH1208/Neural-Network-Notes/blob/main/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BC%98%E5%8C%96%E5%99%A8/%E4%BC%98%E5%8C%96%E5%99%A8/GD.png" alt="Loss over Iteration using Gradient Descent">
  <p>Diagram 1: Loss over Iteration using Gradient Descent</p>
</div>

# 随机梯度下降 Stochastic Gradient Descent 
- 一次随机使用一个实例点进行梯度更新，但是每一次迭代一定会用完所有的实例点
- 当 `batch_size` 大于 `1` 时为小批量随机梯度下降，即使用随机几个实例点进行梯度更新

```python
import numpy as np
import matplotlib.pyplot as plt

def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100, batch_size=1):
    m, n = X.shape
    theta = np.zeros((n, 1))
    loss_history = []

    for _ in range(epochs):
        indices = np.random.choice(m, batch_size, replace=False)
        X_batch, y_batch = X[indices], y[indices]

        predictions = X_batch.dot(theta)
        errors = predictions - y_batch
        gradient = X_batch.T.dot(errors) / batch_size
        theta -= learning_rate * gradient
        loss = np.mean(errors**2) / 2
        loss_history.append(loss)

    return theta, loss_history

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add a bias term to the feature matrix
X_b = np.c_[np.ones((100, 1)), X]

# Run stochastic gradient descent
theta_sgd, loss_history_sgd = stochastic_gradient_descent(X_b, y)

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history_sgd, label='Stochastic Gradient Descent')
plt.title('Loss over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

<div align="center">
  <img src="https://github.com/YapWH1208/Neural-Network-Notes/blob/main/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BC%98%E5%8C%96%E5%99%A8/%E4%BC%98%E5%8C%96%E5%99%A8/SGD.png" alt="Loss over Iteration using Stochastic Gradient Descent">
  <p>Diagram 2: Loss over Iteration using Stochastic Gradient Descent</p>
</div>

# Adam
- 相比于梯度下降，Adam增加了动量的概念，这使得优化过程会更加平滑且稳定
- 另外，Adam也增加了自适应学习率以控制学习过程的平滑度

```python
import numpy as np
import matplotlib.pyplot as plt

def adam_optimizer(X, y, learning_rate=0.01, epochs=100, beta1=0.9, beta2=0.999, epsilon=1e-8):
    m, n = X.shape
    theta = np.zeros((n, 1))
    m_t = np.zeros_like(theta)
    v_t = np.zeros_like(theta)
    t = 0
    loss_history = []

    for epoch in range(epochs):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = X.T.dot(errors)
        
        t += 1
        m_t = beta1 * m_t + (1 - beta1) * gradient
        v_t = beta2 * v_t + (1 - beta2) * (gradient ** 2)

        m_t_hat = m_t / (1 - beta1 ** t)
        v_t_hat = v_t / (1 - beta2 ** t)

        theta -= learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)

        loss = np.mean(errors**2) / 2
        loss_history.append(loss)

    return theta, loss_history

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(1000, 1)
y = 4 + 3 * X + np.random.randn(1000, 1)

# Add a bias term to the feature matrix
X_b = np.c_[np.ones((1000, 1)), X]

# Run Adam optimizer
theta_adam, loss_history_adam = adam_optimizer(X_b, y, epochs=1000)

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history_adam, label='Adam Optimizer')
plt.title('Loss over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

<div align="center">
  <img src="https://github.com/YapWH1208/Neural-Network-Notes/blob/main/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BC%98%E5%8C%96%E5%99%A8/%E4%BC%98%E5%8C%96%E5%99%A8/Adam.png" alt="Loss over Iteration using Adam">
  <p>Diagram 3: Loss over Iteration using Adam</p>
</div>

# 比较
| 优化方法               | 更新规则                           | 优点                       | 缺点                          |
|------------------------|------------------------------------|----------------------------|-------------------------------|
| 梯度下降（GD）          | 使用整个训练集的梯度进行更新        | 简单直观                  | 计算成本高，特别是在大规模数据集上  |
| 随机梯度下降（SGD）    | 使用随机选择的单个样本的梯度更新    | 更新速度更快，适用于大数据集 | 参数更新存在较大方差，可能引入噪声  |
| Adam                   | 结合了动量和自适应学习率的更新规则   | 收敛速度相对较快，适用于不同参数和数据集 | 对超参数敏感，相对复杂          |

<div align="center">
  <img src="https://github.com/YapWH1208/Neural-Network-Notes/blob/main/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%BC%98%E5%8C%96%E5%99%A8/%E4%BC%98%E5%8C%96%E5%99%A8/Comparison.png" alt="Comparison between three optimizer">
  <p>Diagram 4: Comparison between three optimizer</p>
</div>
