> 纸上得来终觉浅，绝知此事要躬行

# 寻找合适的学习率
## 学习率
- 用于控制模型在每次迭代中更新的步长
- 一个模型的学习率可以在不同时步拥有不同的合适值
- 唯一的解决方案就是在训练中不断的寻找最合适当前状态的学习率

## 学习率
1. 较大的学习率加速了网络的训练，但可能无法达到最优解。学习率过大的情况下，网络无法学习到有效的知识。
2. 较小的学习率会使网络训练缓慢。除此之外，过小的学习率可能会使得网络陷入局部最优解。

> 策略：在网络训练初期使用大的学习率加速收敛，之后降低学习率提高模型训练效果。这被称为学习率衰减。

![[Pasted image 20231231092406.png|center|500]]<center>图 1：学习率在不同时步对损失的影响</center>

## 学习率与Batch-Size的关系
Batch size过小：
- 迭代的梯度不平滑，模型训练loss震荡。模型的训练更偏重于拟合个体，导致在训练过程中模型容易忽视数据的整体规律性。
- 训练时长增加
- 内存利用率低

Batch size过大：
- 容易陷入局部最优，从而影响模型效果。过大的Batch Size容易忽视数据中的个体差异性，并使得模型的梯度下降方向固定。
- 内存容易溢出。在实际训练中，如果新进程导致了额外的内存占用，容易强行终止模型训练。

> 策略：Batch-Size的大小与学习率成正比；在显存允许的情况下，最好使用较大的Batch-Size进行训练

需要强调的是，<font color="#ffff00">大的Batch Size会降低模型精度</font>，但<font color="#ffff00">模型的梯度下降方向更为准确</font>，所以辅以设置<font color="#ffff00">更大的学习率可以加速模型的收敛</font>；<font color="#ffff00">小的Batch Size可以更好的捕获到模型的个体差异</font>，从而具有较高的模型精度，并且应该设置更小的学习率去环节<font color="#ffff00">loss震荡的问题</font>。

> OpenAI对于Batch-Size的研究[[1812.06162] An Empirical Model of Large-Batch Training (arxiv.org)](https://arxiv.org/abs/1812.06162)

# Dropout
- 对于每次训练迭代，Dropout会随机选择一部分神经元，并将他们的输出值设置为零。
- 通常在<font color="#ffff00">全连接层</font>使用dropout，在卷积层不使用，注意Dropout并不适合所有的情况。
- 在机器学习和深度学习中，dropout是一种正则化技术，用于减少神经网络的过拟合。
- 使用dropout的主要优势包括：
	- 减小过拟合的风险
	- 增强泛化能力
	- 提高模型的性能

# 迁移学习
- 迁移学习是利用很多预训练的经典模型直接训练我们自己的任务，虽然说领域不同，但是在学习权重的广度方面，两个任务之间还是有联系的。

# 尝试过拟合一个小数据集
- 关闭正则化、Dropout、数据增强、使用训练集一小部分，让神经网络训练几个周期。确保可以实现零损失。
- 这样可以确保神经网络确实可以从数据集中学习

# 交叉验证
- 交叉验证是一种用于评估模型性能和减少过拟合的统计学方法。它将数据集划分为多个子集，然后进行多次模型训练和评估，<font color="#ffff00">每次使用不同的子集作为训练集和测试集。这样可以更全面地评估模型在不同数据子集上的表现</font>。

# Label-Smoothing
- <font color="#ffff00">标签平滑通过在训练集的标签中引入一些噪声，使得模型更具有鲁棒性。</font>

![[Pasted image 20231231110115.png|center|600]]<center>图 2：传统标签对比标签平滑</center>


# 数据集增广/数据集增强
- 数据集增广有多种方式，裁剪取样，旋转，翻转，加噪等，还有cutout， random erasing, mixup training等。

# CNN调优总结
1. 当研究增加训练集大小的时候，需要确定数据集对性能提升的平衡点。
2. 数据质量比数据的数量更重要。
3. 如果网络结构比较复杂， 且经过高度优化，如GoogleNet，建议不要进行修改。

# DNN小技巧
1. Shuffle
2. 扩充数据集
3. 在训练之前，先在非常小的子数据集上训练进行过拟合，铜鼓哦这样来验证网络是可以收敛的，网络结构没有问题。
4. 使用Dropout避免过拟合
5. 避免LRN池化，MAX池化会更快
6. 网络越深，尽可能使用ReLU或者LeakyRelu，而不是sigmoid, tanh
7. 尽可能使用xavier初始化

# 权重初始化
- 尝试所有不同的初始化方法，考察是否有一种方法在其他情况不变的情况下（效果）更优。
- 尝试用无监督的方法，例如自动编码，来进行预先学习。
- 自监督
- 尝试使用一个已经存在的模型，针对你的问题重新训练输入层和输出层。

# Early Stopping
- 一旦训练过程中出现（验证集）性能开始下降，你可以停止训练与学习。这可以节省很多时间。
- 早停法可以有效的避免在训练数据上的过拟合的正则化方式。
- 如果某个条件满足（衡量准确率的损失），你还可以设置检查点(Checkpointing)来储存模型。

