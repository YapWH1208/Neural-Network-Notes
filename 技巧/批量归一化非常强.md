# 理论
- Batch Normalization 是对每个单独的batch在经过线性变换后进行归一化以确保范围可控

| 好处                 | 坏处               |
| -------------------- | ------------------ |
| 加速神经网络收敛速度 | 样本才有效         |
|                      | 对于序列数据效果差 |
|                      | 分布式训练影响效率 |
- 优化法：
	- Layer Normalization：对样本特征空间归一化
	- Instance Normalization：对样本特征空间内逐通道归一化
	- Group Normalization：LN和IN的结合体
$$\large Norm(Z_i) = \gamma{{Z_{i}-\mu}\over{\sqrt{\sigma^2+\epsilon}}}+\beta$$

# 参考文献
- [[5分钟深度学习] #06 批量归一化 Batch Normalization](https://www.bilibili.com/video/BV12d4y1f74C/?vd_source=82cc9f8195ff57b14f4f1d470824ef31)