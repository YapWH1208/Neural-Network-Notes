# 理论
## 1. 回归
将实例点（数据）用一条**线**（不一定是直线）来表达它们的关系，可以理解为找适合它们的万能公式。
## 2. 线性
- 实例点看起来是一条直线
- 这个直线不一定是二维的，可以是更高维的直线
## 3. 线性回归
- 用一条**直线**来表达实例点的关系
- 主要可分为：简单一元线性回归，多元线性回归
### 壹、简单一元线性回归
- 由一元线性方程演变而来
$$\large y = wx +b$$
- 使用机器学习概念参数 $w$ 和 $b$ 来表达，即为：
$$\large \hat y = w_0 + w_1x + \epsilon$$
### 贰、多元线性回归
当数据拥有更多的特征（维度）
$$\large \hat y_i = w_0 + w_1x_1 + w_2x_2 + ...+w_ix_i$$
如果我们利用矩阵形式的表达式
$$\large \hat y = X^Tw$$其中，
$$y=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}$$
$$X = 
\begin{bmatrix}
x_{11}&x_{12}&\cdots&x_{1n}\\
x_{21}&x_{22}&\cdots&x_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
x_{m1}&x_{m2}&\cdots&x_{mn}
\end{bmatrix}$$
$$w=\begin{pmatrix}w_1\\w_2\\\vdots\\w_n\end{pmatrix}$$
## 4. 损失函数
在我们寻找最合适的那条直线时，我们该如何判断那一条才是最合适的？
- 最简单的方法就是使用平方差（Squared Difference）公式
- 更为准确的是残差平方和（Sum of Square Error）

> 以一元线性回归问题为例，其残差平方和的公式为 
> $$\begin{align}Q&=\sum^n_i(y_i-\hat y_i)^2\\&=\sum^n_i(y_i-(\hat w_0+\hat w_1x_i)^2)\end{align}$$
> 该公式表示了其残差为真实值和预测值误差的平方总和

问：为什么要加一个平方？
答：为了消去负数带来的影响

## 5. 最小二乘法
- 由**残差平方和**演变而来
- 对于简单一元线性回归，仍保持最基础的残差平方和公式
- 而对于多元线性回归，我们以矩阵形式的表达式便是
$$\large SSE = (y -Xw)^T(y - Xw)$$
	这个形式是寻找一条直线残差平方和
- 如果想寻找一条最合适的直线，通过求导求出极限值$\frac{\delta}{\delta w}=0$，我们得出
$$\large w = (X^TX)^{-1}X^Ty$$
	当 $(X^TX)^{-1}$ 为可逆矩阵时，但是当我们的实例矩阵不是满秩矩阵时，我们会遇上最小二乘求解结果并非唯一解的情况，因此对此算法进行优化

---
# 变化
## 岭回归（Ridge Regression）
- 为了使实例矩阵 $(X^TX)^{-1}$ 为可逆矩阵，我们在其中增加了一个单位矩阵$I$, 从而得到下列新的公式$$\large w = (X^TX+\lambda I)^{-1}X^Ty$$
- 可逆矩阵的必要条件是不存在多重共线性（同时有多个解）
- 这个方法也被称为L2正则化

## Lasso回归（Lasso Regression）
- 不同于L2正则化，Lasso回归里增加了一个带约束条件的损失函数，从此得到$$\large w = (X^TX)^{-1}X^Ty + \lambda\sum|w_i|$$
- 这个方法被称为L1正则化

# 参考文献
- [机器学习| 算法笔记-线性回归（Linear Regression）](https://zhuanlan.zhihu.com/p/139445419)
- [用人话讲明白线性回归LinearRegression](https://zhuanlan.zhihu.com/p/72513104)

